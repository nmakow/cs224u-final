{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Deep Model\n",
    "\n",
    "Ultimately we want to work in tf to build a model, as it will give the most flexibility and we can build off of some modules we built for SQuAD. \n",
    "\n",
    "We'll go through as follows:\n",
    "0. Set-up.\n",
    "1. Read in dataset.\n",
    "2. Convert dataset into format for RNN.\n",
    "3. Construct vocabulary for RNN.\n",
    "4. Fit TF-RNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmakow/anaconda3/envs/nlu/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# 0. Some initial set-up.\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from tf_rnn_classifier import TfRNNClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report, roc_auc_score\n",
    "import tensorflow as tf\n",
    "import sst\n",
    "from utils import evaluate, build_rnn_dataset\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsmdata_home = 'vsmdata'\n",
    "\n",
    "glove_home = os.path.join(vsmdata_home, 'glove.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15321837663203353070, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 4917166080\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17279758261332026128\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_dir + \"train.csv\").fillna(' ')\n",
    "test = pd.read_csv(data_dir + \"test.csv\").fillna(' ')\n",
    "test_labels = pd.read_csv(data_dir + \"test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_text = train['comment_text']\n",
    "train_labels = train[label_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting dataset for RNN\n",
    "\n",
    "In order to format the dataset for the RNN, we want to format it so that we have a list of lists. Outer list corresponds to training examples, inner list corresponds to token within each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_rnn, Y_rnn = build_rnn_dataset(train, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tYOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE A MOTHJER FUCKER COCKSUCKER! YOU ARE'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(X_rnn['train'][30][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 1, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_rnn['train'][30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get vocab for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_vocab = sst.get_vocab(X_rnn['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst_full_train_vocab has 494,751 items\n"
     ]
    }
   ],
   "source": [
    "print(\"sst_full_train_vocab has {:,} items\".format(len(full_train_vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #1: Vanilla LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 100 #len(X_rnn[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rnn = TfRNNClassifier(\n",
    "    full_train_vocab,\n",
    "    embed_dim=50,\n",
    "    hidden_dim=50,\n",
    "    max_length=100,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=True,\n",
    "    max_iter=10,\n",
    "    eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [2,?,50], [50,6].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1566\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1567\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [2,?,50], [50,6].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ac3dbe41fd73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cs224u-final/tf_model_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Build the computation graph. This method is instantiated by\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# individual subclasses. It defines the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Optimizer set-up:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs224u-final/tf_rnn_classifier.py\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    210\u001b[0m                 self.hidden_dim, self.output_dim, 'W_hy')\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b_y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[0;32m-> 2122\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   2123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   4277\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4278\u001b[0m         \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4279\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4280\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4281\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3390\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3391\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3392\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3394\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1732\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1733\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1734\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1735\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlu/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 2 but is rank 3 for 'MatMul' (op: 'MatMul') with input shapes: [2,?,50], [50,6]."
     ]
    }
   ],
   "source": [
    "_ = tf_rnn.fit(X_rnn['train'][:num_train], Y_rnn['train'][:num_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_rnn_predictions = tf_rnn.predict(X_rnn['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p, r, f1: 0.6711, 0.5515, 0.6054\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p, r, f1: 0.5000, 0.0633, 0.1124\n",
      "\n",
      "CLASS: obscene\n",
      "p, r, f1: 0.6467, 0.5509, 0.5950\n",
      "\n",
      "CLASS: threat\n",
      "p, r, f1: 0.0000, 0.0000, 0.0000\n",
      "\n",
      "CLASS: insult\n",
      "p, r, f1: 0.5981, 0.4624, 0.5216\n",
      "\n",
      "CLASS: identity_hate\n",
      "p, r, f1: 0.2500, 0.0286, 0.0513\n",
      "\n",
      "average F1 score: 0.314273\n",
      "macro-averaged ROC-AUC score: 0.906019\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['dev'][:], tf_rnn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #2: Bidirectional RNN\n",
    "\n",
    "What we'll do is add a flag to the model to be bidirectional. At least initially, the bw cell will use the same hyperparams as the fw cell. Further, we use a separate bw cell vs. fw cell as we feel that ordering will likely make a difference, thus it does not necessarily make sense to have the bw cell share weights with the fw cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_rnn = TfRNNClassifier(\n",
    "    full_train_vocab,\n",
    "    embed_dim=50,\n",
    "    hidden_dim=50,\n",
    "    max_length=50,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=True,\n",
    "    max_iter=50,\n",
    "    bidir_rnn=True, # Bidirectional RNN!\n",
    "    eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 50: loss: 0.035049978643655786"
     ]
    }
   ],
   "source": [
    "num_train=1000#len(X_rnn[\"train\"])\n",
    "\n",
    "_ = bidir_rnn.fit(X_rnn['train'][:num_train], Y_rnn['train'][:num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_rnn_predictionsSMALL = bidir_rnn.predict(X_rnn['train'][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p, r, f1: 0.9388, 0.8846, 0.9109\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p, r, f1: 1.0000, 0.0769, 0.1429\n",
      "\n",
      "CLASS: obscene\n",
      "p, r, f1: 0.8814, 0.8667, 0.8739\n",
      "\n",
      "CLASS: threat\n",
      "p, r, f1: 0.0000, 0.0000, 0.0000\n",
      "\n",
      "CLASS: insult\n",
      "p, r, f1: 0.8070, 0.8519, 0.8288\n",
      "\n",
      "CLASS: identity_hate\n",
      "p, r, f1: 1.0000, 0.1250, 0.2222\n",
      "\n",
      "average F1 score: 0.496458\n",
      "macro-averaged ROC-AUC score: 0.991964\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['train'][:1000], bidir_rnn_predictionsSMALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_rnn_predictions = bidir_rnn.predict(X_rnn['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p, r, f1: 0.7414, 0.6710, 0.7044\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p, r, f1: 0.5000, 0.3165, 0.3876\n",
      "\n",
      "CLASS: obscene\n",
      "p, r, f1: 0.8255, 0.6527, 0.7290\n",
      "\n",
      "CLASS: threat\n",
      "p, r, f1: 0.4444, 0.1404, 0.2133\n",
      "\n",
      "CLASS: insult\n",
      "p, r, f1: 0.6961, 0.6128, 0.6518\n",
      "\n",
      "CLASS: identity_hate\n",
      "p, r, f1: 0.2444, 0.0786, 0.1189\n",
      "\n",
      "average F1 score: 0.467509\n",
      "macro-averaged ROC-AUC score: 0.934755\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['dev'][:], bidir_rnn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #3: GloVe Pretrained Embeddings\n",
    "\n",
    "It seems natural that some pretrained embeddings would inject useful syntactic and semantic information into our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_lookup = utils.glove2dict(\n",
    "    os.path.join(vsmdata_home, 'glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix contains 55422 words.\n"
     ]
    }
   ],
   "source": [
    "glove_vocab = sorted(set(glove_lookup) & set(full_train_vocab))\n",
    "print(\"Embedding matrix contains %d words.\" % len(glove_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = np.array([glove_lookup[w] for w in glove_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab.append(\"$UNK\")\n",
    "glove_embedding = np.vstack(\n",
    "    (glove_embedding, utils.randvec(glove_embedding.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w/o retraining GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was using the limited vocab (only those where glove was defined)\n",
    "bidir_glove_rnn = TfRNNClassifier(\n",
    "    glove_vocab,\n",
    "    embedding=glove_embedding,\n",
    "    embed_dim=100,\n",
    "    hidden_dim=50,\n",
    "    max_length=50,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=False,\n",
    "    max_iter=10,\n",
    "    bidir_rnn=True, # Bidirectional RNN!\n",
    "    eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 10: loss: 8.846844404935837"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_rnn_classifier.TfRNNClassifier at 0x7f363102d7f0>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(X_rnn['train'])\n",
    "bidir_glove_rnn.fit(X_rnn['train'][:num_train], Y_rnn['train'][:num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_glove_rnn_predictions = bidir_glove_rnn.predict(X_rnn['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p: 0.7891 , r: 0.5411, f1: 0.6419\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p: 0.4878 , r: 0.1266, f1: 0.2010\n",
      "\n",
      "CLASS: obscene\n",
      "p: 0.8003 , r: 0.5260, f1: 0.6348\n",
      "\n",
      "CLASS: threat\n",
      "p: 0.6923 , r: 0.1579, f1: 0.2571\n",
      "\n",
      "CLASS: insult\n",
      "p: 0.7036 , r: 0.4538, f1: 0.5517\n",
      "\n",
      "CLASS: identity_hate\n",
      "p: 0.4651 , r: 0.1429, f1: 0.2186\n",
      "\n",
      "average F1 score: 0.417535\n",
      "macro-averaged ROC-AUC score: 0.944858\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['dev'][:], bidir_glove_rnn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w/ retraining GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was using the limited vocab (only those where glove was defined)\n",
    "bidir_glove_retrain_rnn = TfRNNClassifier(\n",
    "    glove_vocab,\n",
    "    embedding=glove_embedding,\n",
    "    embed_dim=50,\n",
    "    hidden_dim=50,\n",
    "    max_length=50,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=True,\n",
    "    max_iter=10,\n",
    "    bidir_rnn=True, # Bidirectional RNN!\n",
    "    eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-19428220a546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbidir_glove_retrain_rnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "num_train = len(X_rnn['train'])\n",
    "bidir_glove_retrain_rnn.fit(X_rnn['train'][:num_train], Y_rnn['train'][:num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_glove_retrain_rnn_predictions = bidir_glove_retrain_rnn.predict(X_rnn['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p: 0.6615 , r: 0.5711, f1: 0.6130\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p: 0.4286 , r: 0.2468, f1: 0.3133\n",
      "\n",
      "CLASS: obscene\n",
      "p: 0.6743 , r: 0.4989, f1: 0.5735\n",
      "\n",
      "CLASS: threat\n",
      "p: 0.5600 , r: 0.2456, f1: 0.3415\n",
      "\n",
      "CLASS: insult\n",
      "p: 0.5993 , r: 0.4316, f1: 0.5018\n",
      "\n",
      "CLASS: identity_hate\n",
      "p: 0.3934 , r: 0.1714, f1: 0.2388\n",
      "\n",
      "average F1 score: 0.430290\n",
      "macro-averaged ROC-AUC score: 0.925147\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['dev'][:], bidir_glove_retrain_rnn_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #4: GloVe + Full Vocab (random for others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix contains 494751 words.\n"
     ]
    }
   ],
   "source": [
    "full_glove_vocab = sorted(set(full_train_vocab))\n",
    "print(\"Embedding matrix contains %d words.\" % len(full_glove_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_glove_embedding = np.array([\n",
    "    glove_lookup[w] \n",
    "    if w in glove_lookup else utils.randvec(len(glove_lookup[\"hello\"])) \n",
    "    for w in full_glove_vocab\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_glove_vocab.append(\"$UNK\")\n",
    "full_glove_embedding = np.vstack(\n",
    "    (full_glove_embedding, utils.randvec(full_glove_embedding.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_full_glove_rnn = TfRNNClassifier(\n",
    "    full_glove_vocab,\n",
    "    embedding=full_glove_embedding,\n",
    "    embed_dim=100,\n",
    "    hidden_dim=50,\n",
    "    max_length=50,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=True,\n",
    "    max_iter=20,\n",
    "    bidir_rnn=True, # Bidirectional RNN!\n",
    "    eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 20: loss: 0.11288791222614236"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_rnn_classifier.TfRNNClassifier at 0x7f362f565f28>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(X_rnn['train'])\n",
    "bidir_full_glove_rnn.fit(X_rnn['train'][:num_train], Y_rnn['train'][:num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "bidir_full_glove_predictions = bidir_full_glove_rnn.predict(X_rnn['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p: 0.7413 , r: 0.6636, f1: 0.7003\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p: 0.4352 , r: 0.2975, f1: 0.3534\n",
      "\n",
      "CLASS: obscene\n",
      "p: 0.8163 , r: 0.6686, f1: 0.7351\n",
      "\n",
      "CLASS: threat\n",
      "p: 0.5000 , r: 0.2456, f1: 0.3294\n",
      "\n",
      "CLASS: insult\n",
      "p: 0.6883 , r: 0.6017, f1: 0.6421\n",
      "\n",
      "CLASS: identity_hate\n",
      "p: 0.3152 , r: 0.2071, f1: 0.2500\n",
      "\n",
      "average F1 score: 0.501711\n",
      "macro-averaged ROC-AUC score: 0.929445\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['dev'][:], bidir_full_glove_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #5: Stacked RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_rnn = TfRNNClassifier(\n",
    "    full_glove_vocab,\n",
    "    embedding=full_glove_embedding,\n",
    "    embed_dim=100,\n",
    "    hidden_dim=50,\n",
    "    max_length=100,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.LSTMCell,\n",
    "    train_embedding=True,\n",
    "    max_iter=10,\n",
    "    bidir_rnn=True, # Bidirectional RNN!\n",
    "    stacked=True, # Stacked RNN!\n",
    "    eta=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 10: loss: 3.178255107253794"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_rnn_classifier.TfRNNClassifier at 0x7f34beaed198>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train = len(X_rnn['train'])\n",
    "stacked_rnn.fit(X_rnn['train'][:num_train], Y_rnn['train'][:num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions = stacked_rnn.predict(X_rnn['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p: 0.7284 , r: 0.6703, f1: 0.6981\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p: 0.5325 , r: 0.2595, f1: 0.3489\n",
      "\n",
      "CLASS: obscene\n",
      "p: 0.8439 , r: 0.6787, f1: 0.7524\n",
      "\n",
      "CLASS: threat\n",
      "p: 0.0000 , r: 0.0000, f1: 0.0000\n",
      "\n",
      "CLASS: insult\n",
      "p: 0.7327 , r: 0.5105, f1: 0.6017\n",
      "\n",
      "CLASS: identity_hate\n",
      "p: 0.3261 , r: 0.1071, f1: 0.1613\n",
      "\n",
      "average F1 score: 0.427079\n",
      "macro-averaged ROC-AUC score: 0.956077\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['dev'][:], stacked_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_predictions_train = stacked_rnn.predict(X_rnn['train'][100000:120000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p: 0.9524 , r: 0.9479, f1: 0.9501\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p: 0.7674 , r: 0.4950, f1: 0.6018\n",
      "\n",
      "CLASS: obscene\n",
      "p: 0.9306 , r: 0.9090, f1: 0.9197\n",
      "\n",
      "CLASS: threat\n",
      "p: 0.0000 , r: 0.0000, f1: 0.0000\n",
      "\n",
      "CLASS: insult\n",
      "p: 0.8914 , r: 0.8467, f1: 0.8684\n",
      "\n",
      "CLASS: identity_hate\n",
      "p: 0.7119 , r: 0.4828, f1: 0.5753\n",
      "\n",
      "average F1 score: 0.652574\n",
      "macro-averaged ROC-AUC score: 0.994512\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['train'][100000:120000], stacked_predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #6: Character Embeddings w/ Char-Level CNN, (w/ GloVe + randoms full vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_emb_rnn = TfRNNClassifier(\n",
    "    full_glove_vocab,\n",
    "    embedding=full_glove_embedding,\n",
    "    embed_dim=100,\n",
    "    hidden_dim=50,\n",
    "    max_length=100,\n",
    "    hidden_activation=tf.nn.tanh,\n",
    "    cell_class=tf.nn.rnn_cell.GRUCell,\n",
    "    train_embedding=True,\n",
    "    char_embed=True,\n",
    "    char_embed_dim=20,\n",
    "    max_iter=50,\n",
    "    word_length=12,\n",
    "    bidir_rnn=True, # Bidirectional RNN!\n",
    "    eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "20\n",
      "_define_embedding: 77, 20\n"
     ]
    }
   ],
   "source": [
    "num_train = len(X_rnn['train'])\n",
    "char_emb_rnn.fit(X_rnn['train'][:num_train], Y_rnn['train'][:num_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_preds = char_emb_rnn.predict(X_rnn['dev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS: toxic\n",
      "p, r, f1: 0.6063, 0.5398, 0.5712\n",
      "\n",
      "CLASS: severe_toxic\n",
      "p, r, f1: 1.0000, 0.0190, 0.0373\n",
      "\n",
      "CLASS: obscene\n",
      "p, r, f1: 0.8139, 0.3314, 0.4711\n",
      "\n",
      "CLASS: threat\n",
      "p, r, f1: 0.0000, 0.0000, 0.0000\n",
      "\n",
      "CLASS: insult\n",
      "p, r, f1: 0.7500, 0.2996, 0.4282\n",
      "\n",
      "CLASS: identity_hate\n",
      "p, r, f1: 0.0000, 0.0000, 0.0000\n",
      "\n",
      "average F1 score: 0.251279\n",
      "macro-averaged ROC-AUC score: 0.914805\n"
     ]
    }
   ],
   "source": [
    "evaluate(Y_rnn['dev'], char_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_preds_t = char_emb_rnn.predict(X_rnn['train'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(Y_rnn['train'][:100], char_preds_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
